# -----Weekly Report for the Week 02/25/2025-----

**Name**: Brett Piggott  
**Report Date**: 03/04/2025  

---

## 1. Weekly Progress

### Papers Read
1. **MEM: Agentic Memory for LLM Agents**  
   *Location*: [Summarizations/A-MEMAgenticMemoryForLLMAgents.md](../Summarizations/A-MEMAgenticMemoryForLLMAgents.md)

2. **A Survey on Self-Evolution of Large Language Models**  
   *Location*: [Summarizations/ASurveyOnSelf-EvolutionOfLargeLanguageModels.md](../Summarizations/ASurveyOnSelf-EvolutionOfLargeLanguageModels.md)

3. **Offline Training of Language Model Agents with Functions as Learnable Weights**  
   *Location*: [Summarizations/OfflineTrainingOfLanguageModelAgentswithFunctionsAsLearnableWeights.md](../Summarizations/OfflineTrainingOfLanguageModelAgentswithFunctionsAsLearnableWeights.md)

### Video Watched
- **Deep Dive into LLMs like ChatGPT by Andrej Karpathy**  
  *Link*: [https://www.youtube.com/watch?v=7xTGNNLPyMI](https://www.youtube.com/watch?v=7xTGNNLPyMI)  
  **Summary**: This was a comprehensive deep dive into Large Language Models (LLMs) covering:
  - Full training stack for ChatGPT-like models
  - Mental models for "psychology" of LLMs
  - Effective prompt-engineering tips
  - Differences in fine-tuning approaches (Supervised vs. RLHF)
  - Various real-world use cases and future directions  

### Dataset Creation
- Began creating a dataset using the **VisDrone** image set ([VisDrone GitHub](https://github.com/VisDrone/VisDrone-Dataset)).
- Generated scene descriptions with **VLM Llava** and performed object detection using **YOLO**.
- Derived tasks based on scene context through **Llama 3.1:8b**.
- Produced Python code via **llama3.3:70B** to complete the dataset items.
- Current JSON format is somewhat tied to a single "scene" concept, missing coverage of different instruction types.  
- **Plan**: Create **25** distinct datasets, each with **300** items (100 simple, 100 intermediate, 100 advanced), merge them, and ensure coverage of diverse instructions/scenarios.

### DeepSeek GitHub Summaries
- Created a [README](../Summarizations/DeepSeekReleases.md) summarizing new DeepSeek GitHub repositories and features.

---

## 2. Challenges
- **High Variance in Prompt Results**: Repeated runs with the same style prompt sometimes yield significantly different responses or syntaxes.
- **Limited Time**: Juggling multiple experiments with minimal time for deeper hyperparameter tuning and refinements.
- **Scalability vs. Latency** As the drone system grows to handle more real-time tasks (e.g., object detection, route planning, environment mapping), simply scaling up the LLM or other modules increases latency. Finding a balance between model size (for quality outputs) and speed (for near real-time operations) is an ongoing challenge.  Hopefully through fine-tuning and quantization we can limit this devide.
---

## 3. Tasks for Next Week
- **Refine Dataset Strategy**: Work on finalizing the plan for building out the 25 separate datasets, ensuring each meets the variety and quantity targets.
- **Improve Syntax Checking**: Continue to refine prompt templates or code to ensure minimal error in the generated minispecs.
- **System Architecture & Demo**: Develop diagrams and prototype demos highlighting how each component (vision, large model, syntax checker) interacts.

---
